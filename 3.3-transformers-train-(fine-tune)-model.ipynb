{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3489a07",
   "metadata": {},
   "source": [
    "# CPV Classifier POC\n",
    "\n",
    "## 3.3 - Fine-Tune Transformers Classifier\n",
    "\n",
    "Simple transformer-based classifier proof of concept based on data from TheyBuyForYou.\n",
    "\n",
    "See https://theybuyforyou.eu/ for background on TheyBuyForYou and http://data.tbfy.eu/ for information on the Knowledge Graph (KG) data that was created as part of this project. Data from the knowledge graph used in this proof of concept is made available under the following license terms and therefore the same license applies to the code and data in this repository.\n",
    "\n",
    "> The KG data is provided under the Creative Commons BY-NC-SA 4.0 License, which allows you to use, share and adapt the data for non-commercial uses as long as you give appropriate credit and share any adapted data under the same license as the original. If you wish to use the data for commercial uses please contact the TheyBuyForYou project.\n",
    "\n",
    "The full CPV listing included in this repo was downloaded from https://simap.ted.europa.eu/cpv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1dcb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 23:18:19.591858: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import shelve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7618ba",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3d5624",
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open(\"data/train_val.shelf\") as db:\n",
    "    sents_train = db[\"sents_train\"]\n",
    "    sents_val = db[\"sents_val\"]\n",
    "    cpv_train = db[\"cpv_train\"]\n",
    "    cpv_val = db[\"cpv_val\"]\n",
    "    label2id = db[\"label2id\"]\n",
    "    id2label = db[\"id2label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfd4ff",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88c35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6397cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(sents_train, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(sents_val, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aacaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CPVDataset(train_encodings, cpv_train)\n",
    "val_dataset = CPVDataset(val_encodings, cpv_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418d5041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(id2label))\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afd702",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf44b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use accuracy metric\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639bba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',          # output directory\n",
    "    num_train_epochs=15,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    save_total_limit=2,\n",
    "    save_steps=10_000,\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10_000,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",             # default optimizer is deprecated now\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d1eb119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 221288\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 207465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207465' max='207465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207465/207465 8:47:46, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.213100</td>\n",
       "      <td>1.549688</td>\n",
       "      <td>0.579510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.483400</td>\n",
       "      <td>1.307204</td>\n",
       "      <td>0.648446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.936200</td>\n",
       "      <td>1.200928</td>\n",
       "      <td>0.699976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>1.190173</td>\n",
       "      <td>0.726777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.557200</td>\n",
       "      <td>1.266526</td>\n",
       "      <td>0.740198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>1.342455</td>\n",
       "      <td>0.757646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>1.471016</td>\n",
       "      <td>0.764275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>1.635881</td>\n",
       "      <td>0.765861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>1.849004</td>\n",
       "      <td>0.773304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>2.022361</td>\n",
       "      <td>0.773182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>2.107410</td>\n",
       "      <td>0.776314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>2.185186</td>\n",
       "      <td>0.778835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>2.238473</td>\n",
       "      <td>0.779689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>2.279716</td>\n",
       "      <td>0.781967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>2.357902</td>\n",
       "      <td>0.782414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-10000\n",
      "Configuration saved in ./output/checkpoint-10000/config.json\n",
      "Model weights saved in ./output/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-20000\n",
      "Configuration saved in ./output/checkpoint-20000/config.json\n",
      "Model weights saved in ./output/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-30000\n",
      "Configuration saved in ./output/checkpoint-30000/config.json\n",
      "Model weights saved in ./output/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-40000\n",
      "Configuration saved in ./output/checkpoint-40000/config.json\n",
      "Model weights saved in ./output/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-50000\n",
      "Configuration saved in ./output/checkpoint-50000/config.json\n",
      "Model weights saved in ./output/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-60000\n",
      "Configuration saved in ./output/checkpoint-60000/config.json\n",
      "Model weights saved in ./output/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-40000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-70000\n",
      "Configuration saved in ./output/checkpoint-70000/config.json\n",
      "Model weights saved in ./output/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-80000\n",
      "Configuration saved in ./output/checkpoint-80000/config.json\n",
      "Model weights saved in ./output/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-60000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-90000\n",
      "Configuration saved in ./output/checkpoint-90000/config.json\n",
      "Model weights saved in ./output/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-70000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-100000\n",
      "Configuration saved in ./output/checkpoint-100000/config.json\n",
      "Model weights saved in ./output/checkpoint-100000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-110000\n",
      "Configuration saved in ./output/checkpoint-110000/config.json\n",
      "Model weights saved in ./output/checkpoint-110000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-90000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-120000\n",
      "Configuration saved in ./output/checkpoint-120000/config.json\n",
      "Model weights saved in ./output/checkpoint-120000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-100000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-130000\n",
      "Configuration saved in ./output/checkpoint-130000/config.json\n",
      "Model weights saved in ./output/checkpoint-130000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-110000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-140000\n",
      "Configuration saved in ./output/checkpoint-140000/config.json\n",
      "Model weights saved in ./output/checkpoint-140000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-120000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-150000\n",
      "Configuration saved in ./output/checkpoint-150000/config.json\n",
      "Model weights saved in ./output/checkpoint-150000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-130000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-160000\n",
      "Configuration saved in ./output/checkpoint-160000/config.json\n",
      "Model weights saved in ./output/checkpoint-160000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-140000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-170000\n",
      "Configuration saved in ./output/checkpoint-170000/config.json\n",
      "Model weights saved in ./output/checkpoint-170000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-150000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-180000\n",
      "Configuration saved in ./output/checkpoint-180000/config.json\n",
      "Model weights saved in ./output/checkpoint-180000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-160000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-190000\n",
      "Configuration saved in ./output/checkpoint-190000/config.json\n",
      "Model weights saved in ./output/checkpoint-190000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-170000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./output/checkpoint-200000\n",
      "Configuration saved in ./output/checkpoint-200000/config.json\n",
      "Model weights saved in ./output/checkpoint-200000/pytorch_model.bin\n",
      "Deleting older checkpoint [output/checkpoint-180000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 8:47:47.526466\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now()\n",
    "trainer.train()\n",
    "finish = datetime.now()\n",
    "\n",
    "print(f\"Completed in {finish - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b5f46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 24588\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='385' max='385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [385/385 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3579022884368896,\n",
       " 'eval_accuracy': 0.7824141857816821,\n",
       " 'eval_runtime': 68.7178,\n",
       " 'eval_samples_per_second': 357.811,\n",
       " 'eval_steps_per_second': 5.603,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcaf3f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./models/transformers/config.json\n",
      "Model weights saved in ./models/transformers/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./models/transformers')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
